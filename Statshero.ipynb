{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ce36f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the file location:C:/Users/int_shansiming/Desktop/Prediction/DailyDelhiClimateTest.csv\n",
      "Please enter your request: analyze the relationship between humidity and wind speed using linear regression\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary packages\n",
    "import openai\n",
    "import runpy\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cleaning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "path = \"C:/Users/int_shansiming/Desktop/Prediction/Nasdaq.csv\"\n",
    "path2 = \"C:/Users/int_shansiming/Desktop/Prediction/data.csv\"\n",
    "path3 = \"C:/Users/int_shansiming/Desktop/Prediction/DailyDelhiClimateTest.csv\"\n",
    "\n",
    "# ------------------------------------------\n",
    "# Set up the parameters for the GPT-3 API\n",
    "model_text = \"text-davinci-002\"\n",
    "model_code = \"text-davinci-002\"\n",
    "temperature_1 = 0.1\n",
    "temperature_2 = 1\n",
    "max_tokens = 3200\n",
    "\n",
    "# ------------------------------------------\n",
    "# Ask for file location\n",
    "user_input_file = input(\"Enter the file location:\");\n",
    "\n",
    "# import the data\n",
    "try:\n",
    "    user_data = cleaning.clean(user_input_file)\n",
    "except ValueError:\n",
    "    user_data = cleaning.clean(user_input_file)\n",
    "\n",
    "# Then get the column names\n",
    "col_name = user_data.columns.tolist()\n",
    "\n",
    "from request_matching import parse_user_input, select_variables_and_model\n",
    "\n",
    "while True:\n",
    "    # Enter request\n",
    "    user_request = input(\"Please enter your request: \")\n",
    "\n",
    "    # Get the desired variables from the input request\n",
    "    parsed_msg = parse_user_input(user_input_file,user_request)\n",
    "    # selected_v = select_variables_and_model(user_input_file, user_request)\n",
    "    selected_v = []\n",
    "    for v in col_name:\n",
    "        if v in parsed_msg:\n",
    "            selected_v.append(v)\n",
    "\n",
    "    # Check if the selected_v set has more than one element\n",
    "    if len(selected_v) > 1:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Please provide a request with at least two variables.\")\n",
    "print(col_name)\n",
    "selected_type = []\n",
    "for v in selected_v:\n",
    "    v_type = user_data[v].dtype\n",
    "    selected_type.append(v_type)\n",
    "\n",
    "features_y = input(f\"Select your y from {selected_v}: \");\n",
    "y_index = selected_v.index(features_y)\n",
    "selected_v.pop(y_index)\n",
    "y_type = selected_type[y_index]\n",
    "selected_type.pop(y_index)\n",
    "\n",
    "prompt = f'''Based on the name and the type of x and y, and the request {parsed_msg}\n",
    "        ,distinguish what is the best statistical model for user's request\n",
    "        the name of x is {selected_v}\n",
    "        the name of y is {features_y}\n",
    "        the type of x is {selected_type} accordingly\n",
    "        the type of y is {y_type}\n",
    "        choose one suitable model based on bias-variance trade-off\n",
    "        you must only print the name of statistical model, no explainations needed\n",
    "        '''\n",
    "response_text = openai.Completion.create(\n",
    "    engine=model_text,\n",
    "    prompt=prompt,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=0.7,\n",
    ")\n",
    "rec_ml = f'''{response_text.choices[0].text.strip()}'''\n",
    "\n",
    "# ------------------------------------------\n",
    "\n",
    "print(\" \")\n",
    "print(\"\\033[1m\\033[4m\\033[36mPlot\\033[0m\")\n",
    "print(\" \")\n",
    "\n",
    "prompt = f'''\n",
    "   Generate Python code to accomplish the following tasks:\n",
    "1. Import cleaning.py and use the cleaning.clean({user_input_file}), save as 'df'.\n",
    "2. Import matplotlib.pyplot as plt and create a plot to display the relationship between x = {selected_v} and y = {features_y}.\n",
    "3. make the size of the plot: plt.figure(figsize=(12, 6))\n",
    "4. Add a title to the graph using the Matplotlib library.\n",
    "5. Label the axes using appropriate units based on the names of the features.\n",
    "6. use plt.show() in the end of the code\n",
    "\n",
    "\n",
    "Please provide the code without any additional comments or notes.\n",
    "    '''\n",
    "    # Generate code using the GPT-3 API\n",
    "response = openai.Completion.create(\n",
    "    engine=model_code,\n",
    "    prompt=prompt,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature_1,\n",
    ")\n",
    "\n",
    "# Save the generated code to a file\n",
    "with open(\"generated_code.py\", \"w\") as f:\n",
    "    f.write(response.choices[0].text.strip())\n",
    "\n",
    "# Import the generated code as a module\n",
    "import generated_code\n",
    "runpy.run_path(\"generated_code.py\")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Using openai api to generate a comprehensive report\n",
    "print(\" \")\n",
    "print(\"\\033[1m\\033[4m\\033[36mIntroduction to the variables\\033[0m\")\n",
    "print(\" \")\n",
    "prompt_text = f'''\n",
    "Using the Excel file {user_input_file}, please provide a brief introduction to the following variables: \n",
    "{col_name}. \n",
    "Please only describe these variables and do not create any new ones. \n",
    "'''\n",
    "\n",
    "response_text = openai.Completion.create(\n",
    "    engine=model_text,\n",
    "    prompt=prompt_text,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature_2,\n",
    ")\n",
    "print(f'''{response_text.choices[0].text.strip()}''')\n",
    "\n",
    "# --------------------------\n",
    "# Generate the descriptive statistics\n",
    "print(\" \")\n",
    "print(\"\\033[1m\\033[4m\\033[36mDescriptive statistics\\033[0m\")\n",
    "print(\" \")\n",
    "from Descriptive_statistics import des_chart\n",
    "\n",
    "des_stats = des_chart(user_input_file)\n",
    "print(tabulate(des_stats, headers='keys', tablefmt='github', showindex=True))\n",
    "print(\" \")\n",
    "# Write a prompt that can read the descriptive statistics\n",
    "prompt_des_stats = f'''\n",
    "Here is a chart of descriptive statistics from the Excel file {user_input_file}:\n",
    "{des_stats}\n",
    "Please provide a detailed description and insights of the main characteristics and patterns in this summary chart\n",
    "In a professional statistician's tongue.\n",
    "'''\n",
    "\n",
    "response_text = openai.Completion.create(\n",
    "    engine=model_text,\n",
    "    prompt=prompt_des_stats,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature_2,\n",
    ")\n",
    "print(f'''{response_text.choices[0].text.strip()}''')\n",
    "\n",
    "# --------------------------\n",
    "# Generate the correlation relationship chart\n",
    "print(\" \")\n",
    "print(\"\\033[1m\\033[4m\\033[36mCorrelation\\033[0m\")\n",
    "print(\" \")\n",
    "from Descriptive_statistics import cor_chart\n",
    "\n",
    "cor_stats = cor_chart(user_input_file)\n",
    "print(tabulate(cor_stats, headers='keys', tablefmt='github', showindex=True))\n",
    "print(\" \")\n",
    "prompt_cor_stats = f'''\n",
    "Here is a chart of correlation from the Excel file {user_input_file}:\n",
    "{cor_stats}\n",
    "Please provide a detailed description and insights of \n",
    "the main characteristics and patterns in this correlation chart,\n",
    "then analyze its insights\n",
    "In a professional statistician's tongue.\n",
    "'''\n",
    "\n",
    "response_text = openai.Completion.create(\n",
    "    engine=model_text,\n",
    "    prompt=prompt_cor_stats,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature_2,\n",
    ")\n",
    "print(f'''{response_text.choices[0].text.strip()}''')\n",
    "\n",
    "def standardize_term(text):\n",
    "    term_mappings = {\n",
    "        \"time series analysis\": [\"time series analysis\", \"time series\", \"ARIMA\", \"SARIMAX\", \"seasonal decomposition\", \"Holt-Winters\"],\n",
    "        \"linear regression\": [\"linear regression\", \"LinearRegression\", \"OLS\", \"ordinary least squares\", \"linear model\"],\n",
    "        \"logistic regression\": [\"logistic regression\", \"LogisticRegression\", \"logit\", \"logistic model\"],\n",
    "        \"correlation analysis\": [\"correlation analysis\", \"correlation\", \"correlation coefficient\", \"Pearson\", \"Spearman\", \"Kendall\"],\n",
    "        \"support vector machines\": [\"support vector machines\", \"SVM\", \"support vector machine\"],\n",
    "        \"decision trees\": [\"decision trees\", \"DecisionTree\", \"decision tree\", \"CART\", \"classification and regression tree\"],\n",
    "        \"random forests\": [\"random forests\", \"RandomForest\", \"random forest\"],\n",
    "        \"gradient boosting\": [\"gradient boosting\", \"GradientBoosting\", \"GBM\", \"XGBoost\", \"LightGBM\", \"CatBoost\"],\n",
    "        \"neural networks\": [\"neural networks\", \"neural network\", \"deep learning\", \"artificial neural network\", \"ANN\"],\n",
    "        \"k-means clustering\": [\"k-means clustering\", \"KMeans\", \"k-means\"],\n",
    "        \"principal component analysis\": [\"principal component analysis\", \"PCA\", \"principal components\"]\n",
    "    }\n",
    "    \n",
    "    for standard_term, synonyms in term_mappings.items():\n",
    "        for syn in synonyms:\n",
    "            if syn.lower() in text.lower():\n",
    "                return standard_term\n",
    "    \n",
    "    return None  # If no match is found, return None\n",
    "\n",
    "# Usage\n",
    "# Replace this with the output from GPT-3\n",
    "\n",
    "standard_method_name = standardize_term(rec_ml)\n",
    "\n",
    "###\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# -------Time Series--------\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# If the data is time series, then whether stationary?\n",
    "if standard_method_name == \"time series analysis\":\n",
    "    print(\" \")\n",
    "    print(\"\\033[1m\\033[4m\\033[36mTime Series Analysis\\033[0m\")\n",
    "    print(\" \")\n",
    "\n",
    "    from ADF_test import is_stationary\n",
    "    from stationary_plot import sta_plt\n",
    "\n",
    "    # Generate rolling mean and sd plot\n",
    "    sta_plt(user_input_file, features_y)\n",
    "\n",
    "    # Generate if stationary True/False\n",
    "    message_st = is_stationary(user_input_file, features_y)\n",
    "\n",
    "    prompt_cor_stats = f'''\n",
    "    Here is a message about whether the Excel file {user_input_file} is stationary about feature {features_y}:\n",
    "    {message_st}\n",
    "    Please provide a detailed description about stationary,and analyze the meaning of the above message in context\n",
    "    in a neat and professional statistician's tongue.\n",
    "    The output should be in fowllowing format:\n",
    "    A stationary time series data means that:\n",
    "    The above graph indicates that the time series is:\n",
    "    '''\n",
    "    response_text = openai.Completion.create(\n",
    "        engine=model_text,\n",
    "        prompt=prompt_cor_stats,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature_2,\n",
    "    )\n",
    "    print(f'''{response_text.choices[0].text.strip()}''')\n",
    "\n",
    "    # Perform ARIMA\n",
    "    from arima import arima_model\n",
    "    arima_model,arima_pred,arima_residuals,arima_summary = arima_model(user_input_file, features_y)\n",
    "    # Use AI to explain the model\n",
    "    prompt_arima = f'''Given an {arima_model} model\n",
    "    please explain the model in detail. Assume that the model has already been fit to a time series dataset.\n",
    "    Please explain the meaning and significance of each of these parameters \n",
    "    Additionally, please explain how the model was fit to the data, \n",
    "    and how the predict() and predict_in_sample() methods can be used to forecast future values of the time series.\n",
    "    Finally, please provide any additional insights or observations about the model that you feel would be helpful in understanding its behavior and performance.\n",
    "    '''\n",
    "    response_text = openai.Completion.create(\n",
    "        engine=model_code,\n",
    "        prompt=prompt_arima,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature_2,\n",
    "    )\n",
    "    print(f'''{response_text.choices[0].text.strip()}''')\n",
    "\n",
    "    prompt_arima_2 = f'''Given a list of residuals from {arima_residuals}, explain how well an ARIMA model has performed:\n",
    "\n",
    "    The ARIMA model was used to forecast a time series. The model generated a list of residuals by subtracting the predicted values from the actual values. Please analyze the list of residuals and provide a detailed report on how well the ARIMA model has performed. Specifically, please answer the following questions:\n",
    "\n",
    "    1. What is the mean of the residuals?\n",
    "    2. What is the standard deviation of the residuals?\n",
    "    3. Are the residuals normally distributed? \n",
    "    4. Is there any evidence of autocorrelation in the residuals? \n",
    "    5. Is there any evidence of heteroscedasticity in the residuals? \n",
    "    '''\n",
    "    response_text = openai.Completion.create(\n",
    "        engine=model_text,\n",
    "        prompt=prompt_arima_2,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature_2,\n",
    "    )\n",
    "    print(f'''{response_text.choices[0].text.strip()}''')\n",
    "\n",
    "    print(arima_summary)\n",
    "    prompt_arima_2 = f'''Please explain the results table for a SARIMAX model generated using the auto_arima function in Python. \n",
    "    The table is displayed in the following format:\n",
    "    {arima_summary}\n",
    "    Please provide a detailed explanation of the table, including what each column and row represents, \n",
    "    and the significance of the coefficients and test statistics. \n",
    "    Based on the value of each coefficients and test statistics, explain how well the model is performing\n",
    "    Additionally, please explain any other relevant information,\n",
    "    such as the model order and how it was determined, and any assumptions that were made in fitting the model.\n",
    "    '''\n",
    "    response_text = openai.Completion.create(\n",
    "        engine=model_text,\n",
    "        prompt=prompt_arima_2,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature_2,\n",
    "    )\n",
    "    print(f'''{response_text.choices[0].text.strip()}''')\n",
    "    # --------------------------\n",
    "    # --------------------------\n",
    "    # --------------------------\n",
    "    # --------------------------\n",
    "elif standard_method_name == \"linear regression\":\n",
    "    print(\" \")\n",
    "    print(\"\\033[1m\\033[4m\\033[36mLinear Regression\\033[0m\")\n",
    "    print(\" \")\n",
    "    # --------------------------\n",
    "    # --------------------------\n",
    "    # ----linear regression-----\n",
    "    # --------------------------\n",
    "    # --------------------------\n",
    "    # Load your dataset\n",
    "    # Set the target variable and feature variables\n",
    "    # Replace 'target_variable' with the column name of the target variable in your dataset\n",
    "    # Replace 'feature1', 'feature2', ... with the column names of the feature variables in your dataset\n",
    "    X = user_data[selected_v]\n",
    "    y = user_data[features_y]\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the test set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate residuals\n",
    "    train_residuals = y_train - y_train_pred\n",
    "    test_residuals = y_test - y_test_pred\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Calculate and visualize residuals\n",
    "    residuals = y_test - y_test_pred\n",
    "    plt.scatter(y_test, residuals, alpha=0.5)\n",
    "    plt.title(\"Test Residuals vs. True Values\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter plot of true values vs. predicted values\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.title(\"True Values vs. Test Predicted Values\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.show()\n",
    "    \n",
    "    coef = model.coef_\n",
    "    Inter = model.intercept_\n",
    "    \n",
    "    # Print essential information\n",
    "    print(\"Coefficients:\", model.coef_)\n",
    "    print(\"Intercept:\", model.intercept_)\n",
    "    print(\"Training MSE:\", train_mse)\n",
    "    print(\"Test MSE:\", test_mse)\n",
    "    print(\"Training R-squared:\", train_r2)\n",
    "    print(\"Test R-squared:\", test_r2)\n",
    "    \n",
    "    prompt = f'''Please analyze the given list of residuals {train_residuals} of training group,\n",
    "    and residuals {test_residuals} from the test group,and identify any trends or patterns that may be present. \n",
    "    Based on the observed trends, determine how the linear regression model is functioning and suggest possible improvements.\n",
    "    '''\n",
    "    response_text = openai.Completion.create(\n",
    "        engine=model_text,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature_2,\n",
    "    )\n",
    "    print(\" \")\n",
    "    print(\"\\033[1m\\033[4m\\033[36mSummary\\033[0m\")\n",
    "    print(\" \")\n",
    "    print(f'''{response_text.choices[0].text.strip()}''')\n",
    "    \n",
    "    prompt = f'''Given the summarized features and performance metrics of a linear regression model,\n",
    "    please analyze the model's performance. Identify any potential issues, such as multicollinearity,\n",
    "    heteroscedasticity, or overfitting. Discuss the model's effectiveness in explaining the variance \n",
    "    in the target variable and suggest possible improvements or alternative modeling approaches.\n",
    "    Coefficients: {coef}\n",
    "    Intercept: {Inter}\n",
    "    Training MSE: {train_mse}\n",
    "    Test MSE:{test_mse}\n",
    "    Training R-squared:{train_r2}\n",
    "    Test R-squared: {test_r2}\n",
    "    \n",
    "    '''\n",
    "    response_text = openai.Completion.create(\n",
    "        engine=model_text,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature_2,\n",
    "    )\n",
    "    print(\" \")\n",
    "    print(\"\\033[1m\\033[4m\\033[36mResidual\\033[0m\")\n",
    "    print(\" \")\n",
    "    print(f'''{response_text.choices[0].text.strip()}''')\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "path = \"C:/Users/int_shansiming/Desktop/Prediction/Nasdaq.csv\"\n",
    "path3 = \"C:/Users/int_shansiming/Desktop/Prediction/DailyDelhiClimateTest.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5cfce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcc1c4a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the file location:C:/Users/int_shansiming/Desktop/Prediction/DailyDelhiClimateTest.csv\n",
      "Please enter your request: I want to predict the humidity based on date\n",
      "['Unnamed: 0.1', 'Unnamed: 0', 'date', 'meantemp', 'humidity', 'wind speed', 'meanpressure']\n",
      "Select your y from ['date', 'humidity']: humidity\n",
      "Error: The DType <class 'numpy.dtype[datetime64]'> could not be promoted by <class 'numpy.dtype[float64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[datetime64]'>, <class 'numpy.dtype[float64]'>)\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary packages\n",
    "import openai\n",
    "import runpy\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import cleaning\n",
    "from cleaning import clean\n",
    "import importlib\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "path = \"C:/Users/int_shansiming/Desktop/Prediction/Nasdaq.csv\"\n",
    "path2 = \"C:/Users/int_shansiming/Desktop/Prediction/data.csv\"\n",
    "path3 = \"C:/Users/int_shansiming/Desktop/Prediction/DailyDelhiClimateTest.csv\"\n",
    "\n",
    "# ------------------------------------------\n",
    "# Set up the parameters for the GPT-3 API\n",
    "model_text = \"text-davinci-002\"\n",
    "model_code = \"text-davinci-002\"\n",
    "temperature_1 = 0.1\n",
    "temperature_2 = 1\n",
    "max_tokens = 3200\n",
    "\n",
    "# ------------------------------------------\n",
    "# Ask for file location\n",
    "user_input_file = input(\"Enter the file location:\");\n",
    "\n",
    "# import the data\n",
    "try:\n",
    "    user_data = cleaning.clean(user_input_file)\n",
    "except ValueError:\n",
    "    user_data = cleaning.clean(user_input_file)\n",
    "\n",
    "# Then get the column names\n",
    "col_name = user_data.columns.tolist()\n",
    "\n",
    "from request_matching import parse_user_input, select_variables_and_model\n",
    "\n",
    "while True:\n",
    "    # Enter request\n",
    "    user_request = input(\"Please enter your request: \")\n",
    "\n",
    "    # Get the desired variables from the input request\n",
    "    parsed_msg = parse_user_input(user_input_file,user_request)\n",
    "    # selected_v = select_variables_and_model(user_input_file, user_request)\n",
    "    selected_v = []\n",
    "    for v in col_name:\n",
    "        if v in parsed_msg:\n",
    "            selected_v.append(v)\n",
    "\n",
    "    # Check if the selected_v set has more than one element\n",
    "    if len(selected_v) > 1:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Please provide a request with at least two variables.\")\n",
    "print(col_name)\n",
    "selected_type = []\n",
    "for v in selected_v:\n",
    "    v_type = user_data[v].dtype\n",
    "    selected_type.append(v_type)\n",
    "\n",
    "features_y = input(f\"Select your y from {selected_v}: \");\n",
    "y_index = selected_v.index(features_y)\n",
    "selected_v.pop(y_index)\n",
    "y_type = selected_type[y_index]\n",
    "selected_type.pop(y_index)\n",
    "\n",
    "success = False\n",
    "attempts = 0\n",
    "while not succuss and attempts <1:\n",
    "    try:\n",
    "        prompt = f'''Based on the name and the type of x and y, and the request {parsed_msg}\n",
    "        ,distinguish what is the best statistical model for user's request\n",
    "        the name of x is {selected_v}\n",
    "        the name of y is {features_y}\n",
    "        the type of x is {selected_type} accordingly\n",
    "        the type of y is {y_type}\n",
    "        choose one suitable model based on bias-variance trade-off\n",
    "        print only the name of one suitable modelchoose the model based on bias-variance trade-off, no explainations needed\n",
    "        '''\n",
    "        response_text = openai.Completion.create(\n",
    "            engine=model_text,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        rec_ml = \"The recommended model for these features is \" + f'''{response_text.choices[0].text.strip()}'''\n",
    "        prompt = f'''Write Python code to:\n",
    "        Import the necessary packages, including the custom cleaning.py module.\n",
    "        Load the dataset using the cleaning.clean function, providing the user's input file {user_input_file} as an argument, and save it as 'df'.\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        Split the dataset into training and testing sets using the dependent variable y represented by {features_y} and the independent variables x represented by {selected_v}.\n",
    "        Fit a model using the {rec_ml} method on the training set.\n",
    "        Generate predictions using the fitted model on the testing set.\n",
    "        Create am appropriate plot of the original data and the predictions using the 'matplotlib.pyplot' library, set the figsize by plt.figure(figsize=(12, 6)).\n",
    "        Add a title to the graph using the Matplotlib library.\n",
    "        Create an residual plot showing the difference between the predictions and testing y values.\n",
    "        Exclude any comments or notes from the code, generate code only.\n",
    "        The color shoule be chosen from ('salmon','tomato','black')\n",
    "        Follow the above instructions step by step, and do not miss any instructions.\n",
    "        '''\n",
    "\n",
    "        # Generate code using the GPT-3 API\n",
    "        response = openai.Completion.create(\n",
    "            engine=model_code,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature_1,\n",
    "        )\n",
    "\n",
    "        # Save the generated code to a file\n",
    "        with open(\"gen_ml_code.py\", \"w\") as f:\n",
    "            f.write(response.choices[0].text.strip())\n",
    "\n",
    "        path_to_script = \"gen_ml_code.py\"\n",
    "        graph_gen = runpy.run_path(path_to_script)\n",
    "        success = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        attempts += 1\n",
    "\n",
    "\n",
    "# analyze the relationship between humidity mean teamperature and wind speed, so that i can predict humidity\n",
    "# C:/Users/int_shansiming/Desktop/Prediction/DailyDelhiClimateTest.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb80f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
